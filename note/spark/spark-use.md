# Sparkå¿«é€Ÿå…¥é—¨ ğŸ•¹ï¸0.1.0

#### å…ˆè¯´ä¸€äº›åºŸè¯  
ä¸ºäº†æ–¹ä¾¿å¹¿å¤§å…¥é—¨èœé¸Ÿä»¬å‡†å¤‡çš„ä¸€ç¯‡å…³äºæ¦‚å¿µç‚¹çš„ç®€å•ä»‹ç»åšå®¢ï¼Œå¸®åŠ©å¤§å®¶å¿«é€Ÿä¸Šæ‰‹Spark  

## æ ¸å¿ƒæ¨¡å—
1. spark core  
	+ [å‚è€ƒæ–‡æ¡£â€”â€”â€”â€”Spark æ ¸å¿ƒç¼–ç¨‹](https://blog.csdn.net/weixin_44966780/article/details/122323347)  
2. spark sql  
3. spark streaming  
4. spark mllib(æœºå™¨å­¦ä¹ )  
5. spark graphx(å›¾å½¢æŒ–æ˜è®¡ç®—)


## åŸºäºScalaçš„è¯­æ³•
[å‚è€ƒæ–‡æ¡£â€”â€”â€”â€”Sparkå¿«é€Ÿä¸Šæ‰‹-WordCountæ¡ˆä¾‹](https://blog.csdn.net/weixin_44480968/article/details/119530464)  
wordcountï¼Œæœ€ç®€å•çš„ä¸Šæ‰‹ï¼Œä¸»è¦æ˜¯åštextæ–‡ä»¶ä¸­çš„å•è¯ç»Ÿè®¡å—ï¼Œæœ€ç»ˆç‰ˆæ˜¯ä½¿ç”¨ç®—å­  
```
package wc

import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkConf, SparkContext}

object Spark01_WordCount {

  def main(args: Array[String]): Unit = {

    //application
    //Sparkæ¡†æ¶ï¼šæ˜¯ä¸€ä¸ªç¯å¢ƒ

    //TODO å»ºç«‹å’Œsparkçš„è¿æ¥
    //JDBC:Connection
    val sparkConf = new SparkConf().setMaster("local").setAppName("WordCount")
    val sc = new SparkContext(sparkConf)

    //TODO æ‰§è¡Œä¸šåŠ¡æ“ä½œ
    //1.è¯»å–æ–‡ä»¶ï¼Œè·å–ä¸€è¡Œä¸€è¡Œçš„æ•°æ®
    var lines: RDD[String] = sc.textFile("datas")

    //2.ä¸€è¡Œæ•°æ®è¿›è¡Œæ‹†åˆ†ï¼Œå½¢æˆä¸€ä¸ªä¸€ä¸ªçš„å•è¯(åˆ†è¯):æ‰å¹³åŒ–
    var words: RDD[String] = lines.flatMap(_.split(" "))

    //3.å°†æ•°æ®æ ¹æ®åˆ†è¯è¿›è¡Œåˆ†ç»„ï¼Œä¾¿äºç»Ÿè®¡
    var wordGroup: RDD[(String, Iterable[String])] = words.groupBy(word => word)

    //4.å¯¹åˆ†ç»„åçš„æ•°æ®è¿›è¡Œè½¬æ¢ï¼ˆç»Ÿè®¡ï¼‰
    val wordToCount = wordGroup.map{
      case (word, list) => {
        (word, list.size)
      }
    }

    //5.å°†è½¬æ¢ç»“æœé‡‡é›†åˆ°æ§åˆ¶å°æ‰“å°å‡ºæ¥
    var varry: Array[(String, Int)] = wordToCount.collect()
    varry.foreach(println)

    //TODO å…³é—­sparkçš„è¿æ¥
    sc.stop()
  }
}
```


## ä¸€äº›æ¦‚å¿µ
### è¿è¡Œæ¨¡å¼ï¼ˆå¦‚ä½•è¿æ¥åˆ°sparké›†ç¾¤ï¼‰ï¼šæœ¬åœ°æ¨¡å¼ï¼Œstandaloneæ¨¡å¼ï¼Œyarnæ¨¡å¼ï¼ˆå·¥ä½œä¸­ï¼‰ï¼Œk8så®¹å™¨æ¨¡å¼ï¼Œwindowsæ¨¡å¼  
![Sparkè¿è¡Œæ¨¡å¼](spark-use_files/Sparkè¿è¡Œæ¨¡å¼.png)
### æäº¤åº”ç”¨ç¨‹åºï¼šä»¥åŠæäº¤æ—¶å€™æ‰€ç”¨åˆ°å‚æ•°--æ‰§è¡Œçš„ä¸»ç±»ï¼Œéƒ¨ç½²æ¨¡å¼ï¼Œè¿è¡Œç±»æ‰€åœ¨çš„jaråŒ…ï¼Œå½“å‰åº”ç”¨çš„ä»»åŠ¡å‚æ•°  
### é…ç½®å†å²æœåŠ¡ï¼šå¦‚æœsparkæœåŠ¡æŒ‚äº†ï¼Œä¹‹å‰çš„ä»»åŠ¡å°±çœ‹ä¸åˆ°äº†ï¼Œæ‰€ä»¥éœ€è¦ä½¿ç”¨å†å²æœåŠ¡  
### é…ç½®é«˜å¯ç”¨ï¼šè§£å†³å•ç‚¹æ•…éšœï¼ŒåŒmasterï¼Œä¸€èˆ¬éƒ½ç”¨zookeeperé…ç½®ï¼Œé«˜å¯ç”¨åˆå«HA  


## æ ¸å¿ƒæ¦‚å¿µ
### Sparké›†ç¾¤è§’è‰²
1. Cluster Managerï¼Œ
	é›†ç¾¤ç®¡ç†å™¨ï¼Œå®ƒå­˜åœ¨äºMasterè¿›ç¨‹ä¸­ï¼Œä¸»è¦ç”¨æ¥å¯¹åº”ç”¨ç¨‹åºç”³è¯·çš„èµ„æºè¿›è¡Œç®¡ç†ï¼Œæ ¹æ®å…¶éƒ¨ç½²æ¨¡å¼çš„ä¸åŒï¼Œå¯ä»¥åˆ†ä¸ºlocalï¼Œstandaloneï¼Œyarnï¼Œmesosç­‰æ¨¡å¼ã€‚
2. workerï¼Œ
	workeræ˜¯sparkçš„å·¥ä½œèŠ‚ç‚¹ï¼Œç”¨äºæ‰§è¡Œä»»åŠ¡çš„æäº¤ï¼Œä¸»è¦å·¥ä½œèŒè´£æœ‰ä¸‹é¢å››ç‚¹ï¼š
	workerèŠ‚ç‚¹é€šè¿‡æ³¨å†Œæœºå‘cluster manageræ±‡æŠ¥è‡ªèº«çš„cpuï¼Œå†…å­˜ç­‰ä¿¡æ¯ã€‚
	worker èŠ‚ç‚¹åœ¨spark masterä½œç”¨ä¸‹åˆ›å»ºå¹¶å¯ç”¨executorï¼Œexecutoræ˜¯çœŸæ­£çš„è®¡ç®—å•å…ƒã€‚
	spark masterå°†ä»»åŠ¡Taskåˆ†é…ç»™workerèŠ‚ç‚¹ä¸Šçš„executorå¹¶æ‰§è¡Œè¿ç”¨ã€‚
	workerèŠ‚ç‚¹åŒæ­¥èµ„æºä¿¡æ¯å’ŒexecutorçŠ¶æ€ä¿¡æ¯ç»™cluster managerã€‚
3. Executorï¼Œ
	executor æ˜¯çœŸæ­£æ‰§è¡Œè®¡ç®—ä»»åŠ¡çš„ç»„ä»¶ï¼Œå®ƒæ˜¯applicationè¿è¡Œåœ¨workerä¸Šçš„ä¸€ä¸ªè¿›ç¨‹ã€‚
	è¿™ä¸ªè¿›ç¨‹è´Ÿè´£Taskçš„è¿è¡Œï¼Œå®ƒèƒ½å¤Ÿå°†æ•°æ®ä¿å­˜åœ¨å†…å­˜æˆ–ç£ç›˜å­˜å‚¨ä¸­ï¼Œä¹Ÿèƒ½å¤Ÿå°†ç»“æœæ•°æ®è¿”å›ç»™Driverã€‚
4. Applicationï¼Œ
	applicationæ˜¯Spark API ç¼–ç¨‹çš„åº”ç”¨ç¨‹åºï¼Œå®ƒåŒ…æ‹¬å®ç°DriveråŠŸèƒ½çš„ä»£ç å’Œåœ¨ç¨‹åºä¸­å„ä¸ªexecutorä¸Šè¦æ‰§è¡Œçš„ä»£ç ï¼Œä¸€ä¸ªapplicationç”±å¤šä¸ªjobç»„æˆã€‚
	å…¶ä¸­åº”ç”¨ç¨‹åºçš„å…¥å£ä¸ºç”¨æˆ·æ‰€å®šä¹‰çš„mainæ–¹æ³•ã€‚
5. Driverï¼Œ
	é©±åŠ¨å™¨èŠ‚ç‚¹ï¼Œå®ƒæ˜¯ä¸€ä¸ªè¿è¡ŒApplicationä¸­mainå‡½æ•°å¹¶åˆ›å»ºSparkContextçš„è¿›ç¨‹ã€‚applicationé€šè¿‡Driver å’ŒCluster ManageråŠexecutorè¿›è¡Œé€šè®¯ã€‚
	å®ƒå¯ä»¥è¿è¡Œåœ¨applicationèŠ‚ç‚¹ä¸Šï¼Œä¹Ÿå¯ä»¥ç”±applicationæäº¤ç»™Cluster Managerï¼Œå†ç”±Cluster Managerå®‰æ’workerè¿›è¡Œè¿è¡Œã€‚
	DriverèŠ‚ç‚¹ä¹Ÿè´Ÿè´£æäº¤Jobï¼Œå¹¶å°†Jobè½¬åŒ–ä¸ºTaskï¼Œåœ¨å„ä¸ªExecutorè¿›ç¨‹é—´åè°ƒTaskçš„è°ƒåº¦ã€‚
6. SparkContextå¯¹è±¡ï¼Œ
	sparkContextæ˜¯æ•´ä¸ªsparkåº”ç”¨ç¨‹åºæœ€å…³é”®çš„ä¸€ä¸ªå¯¹è±¡ï¼Œæ˜¯Sparkæ‰€æœ‰åŠŸèƒ½çš„ä¸»è¦å…¥å£ç‚¹ã€‚
	æ ¸å¿ƒä½œç”¨æ˜¯åˆå§‹åŒ–sparkåº”ç”¨ç¨‹åºæ‰€éœ€è¦çš„ç»„ä»¶ï¼ŒåŒæ—¶è¿˜è´Ÿè´£å‘masterç¨‹åºè¿›è¡Œæ³¨å†Œç­‰ã€‚

### RDD
å®ƒæ˜¯Sparkä¸­æœ€é‡è¦çš„ä¸€ä¸ªæ¦‚å¿µï¼Œæ˜¯å¼¹æ€§åˆ†å¸ƒå¼æ•°æ®é›†ï¼Œæ˜¯ä¸€ç§å®¹é”™çš„ã€å¯ä»¥è¢«å¹¶è¡Œæ“ä½œçš„å…ƒç´ é›†åˆï¼Œæ˜¯Sparkå¯¹æ‰€æœ‰æ•°æ®å¤„ç†çš„ä¸€ç§åŸºæœ¬æŠ½è±¡ã€‚
#### ç±»æ¯”åœŸè±†è–¯ç‰‡ç†è§£RDD  
[å‚è€ƒæ–‡æ¡£â€”â€”â€”â€”ã€Spark Coreã€‘ã€RDDã€‘ã€01ã€‘æ ¸å¿ƒå±æ€§ & æ‰§è¡ŒåŸç†](https://blog.csdn.net/weixin_43589563/article/details/121412814)  
![ç±»æ¯”åœŸè±†è–¯ç‰‡ç†è§£](spark-use_files/ç±»æ¯”åœŸè±†è–¯ç‰‡ç†è§£.png)  
```
åˆšä»åœ°é‡ŒæŒ–å‡ºæ¥çš„åœŸè±†é£Ÿæã€æ¸…æ´—è¿‡åçš„å¹²å‡€åœŸè±†ã€ç”Ÿè–¯ç‰‡ã€çƒ¤ç†Ÿçš„è–¯ç‰‡ï¼Œæµæ°´çº¿ä¸Šè¿™äº›é£Ÿæçš„ä¸åŒå½¢æ€ï¼Œå°±åƒæ˜¯ Spark ä¸­ RDD å¯¹äºä¸åŒæ•°æ®é›†åˆçš„æŠ½è±¡ã€‚
RDD å…·æœ‰ 4 å¤§å±æ€§ï¼Œåˆ†åˆ«æ˜¯ partitionsã€partitionerã€dependencies å’Œ compute å±æ€§ã€‚æ­£å› ä¸ºæœ‰äº†è¿™ 4 å¤§å±æ€§çš„å­˜åœ¨ï¼Œè®© RDD å…·æœ‰åˆ†å¸ƒå¼å’Œå®¹é”™æ€§è¿™ä¸¤å¤§æœ€çªå‡ºçš„ç‰¹æ€§ã€‚
partitions: å›¾ä¸­æ¯ä¸€é¢—åœŸè±†å°±æ˜¯ RDD ä¸­çš„æ•°æ®åˆ†ç‰‡ï¼Œ3 é¢—åœŸè±†ä¸€èµ·å¯¹åº”çš„å°±æ˜¯ RDD çš„ partitions å±æ€§ã€‚
partitioner: æ ¹æ®å°ºå¯¸çš„ä¸åŒï¼Œå³é£Ÿè–¯ç‰‡ä¼šè¢«åˆ’åˆ†åˆ°ä¸åŒçš„æ•°æ®åˆ†ç‰‡ä¸­ã€‚åƒè¿™ç§æ•°æ®åˆ†ç‰‡åˆ’åˆ†è§„åˆ™ï¼Œå¯¹åº”çš„å°±æ˜¯ RDD ä¸­çš„ partitioner å±æ€§ã€‚
dependencies: æ¯ç§é£Ÿæå½¢æ€éƒ½ä¾èµ–äºå‰ä¸€ç§é£Ÿæï¼Œè¿™å°±åƒæ˜¯ RDD ä¸­ dependencies å±æ€§è®°å½•çš„ä¾èµ–å…³ç³»
compute: ä¸åŒç¯èŠ‚çš„åŠ å·¥æ–¹æ³•ï¼Œå¯¹åº”çš„åˆšå¥½å°±æ˜¯ RDD çš„ compute å±æ€§ã€‚
```

#### RDDçš„ç‰¹ç‚¹
1. RDDçš„æ•°æ®å¤„ç†æ–¹å¼ç±»ä¼¼äºIOæµï¼Œä¹Ÿæœ‰è£…é¥°è€…è®¾è®¡æ¨¡å¼
2. RDDçš„æ•°æ®åªæœ‰åœ¨è°ƒç”¨collectæ–¹æ³•æ—¶ï¼Œæ‰ä¼šçœŸæ­£æ‰§è¡Œä¸šåŠ¡é€»è¾‘æ“ä½œ
3. RDDæ˜¯ä¸ä¿å­˜æ•°æ®çš„ï¼Œä½†æ˜¯IOå¯ä»¥ä¸´æ—¶ä¿å­˜ä¸€éƒ¨åˆ†æ•°æ®

#### ä»£ç ä¸­æ˜¯ä¸€ä¸ªæŠ½è±¡ç±»ï¼Œå®ƒä»£è¡¨ä¸€ä¸ªå¼¹æ€§çš„ã€ä¸å¯å˜ã€å¯åˆ†åŒºã€é‡Œé¢çš„å…ƒç´ å¯å¹¶è¡Œè®¡ç®—çš„é›†åˆã€‚
1. å¼¹æ€§
	+ å­˜å‚¨çš„å¼¹æ€§ï¼šå†…å­˜ä¸ç£ç›˜çš„è‡ªåŠ¨åˆ‡æ¢ï¼›
	+ å®¹é”™çš„å¼¹æ€§ï¼šæ•°æ®ä¸¢å¤±å¯ä»¥è‡ªåŠ¨æ¢å¤ï¼›
	+ è®¡ç®—çš„å¼¹æ€§ï¼šè®¡ç®—å‡ºé”™é‡è¯•æœºåˆ¶ï¼›
	+ åˆ†ç‰‡çš„å¼¹æ€§ï¼šå¯æ ¹æ®éœ€è¦é‡æ–°åˆ†ç‰‡ã€‚
2. åˆ†å¸ƒå¼ï¼šæ•°æ®å­˜å‚¨åœ¨å¤§æ•°æ®é›†ç¾¤ä¸åŒèŠ‚ç‚¹ä¸Š
3. æ•°æ®é›†ï¼šRDD å°è£…äº†è®¡ç®—é€»è¾‘ï¼Œå¹¶ä¸ä¿å­˜æ•°æ®
4. æ•°æ®æŠ½è±¡ï¼šRDD æ˜¯ä¸€ä¸ªæŠ½è±¡ç±»ï¼Œéœ€è¦å­ç±»å…·ä½“å®ç°
5. ä¸å¯å˜ï¼šRDD å°è£…äº†è®¡ç®—é€»è¾‘ï¼Œæ˜¯ä¸å¯ä»¥æ”¹å˜çš„ï¼Œæƒ³è¦æ”¹å˜ï¼Œåªèƒ½äº§ç”Ÿæ–°çš„ RDDï¼Œåœ¨æ–°çš„ RDD é‡Œé¢å°è£…è®¡ç®—é€»è¾‘
6. å¯åˆ†åŒºã€å¹¶è¡Œè®¡ç®—ï¼Œ
	é»˜è®¤æƒ…å†µä¸‹ï¼ŒSpark å¯ä»¥å°†ä¸€ä¸ªä½œä¸šåˆ‡åˆ†å¤šä¸ªä»»åŠ¡åï¼Œå‘é€ç»™ Executor èŠ‚ç‚¹å¹¶è¡Œè®¡ç®—ï¼Œè€Œèƒ½å¤Ÿå¹¶è¡Œè®¡ç®—çš„ä»»åŠ¡æ•°é‡æˆ‘ä»¬ç§°ä¹‹ä¸ºå¹¶è¡Œåº¦ã€‚
	è¿™ä¸ªæ•°é‡å¯ä»¥åœ¨æ„å»º RDD æ—¶æŒ‡å®šã€‚è®°ä½ï¼Œè¿™é‡Œçš„å¹¶è¡Œæ‰§è¡Œçš„ä»»åŠ¡æ•°é‡ï¼Œå¹¶ä¸æ˜¯æŒ‡çš„åˆ‡åˆ†ä»»åŠ¡çš„æ•°é‡ï¼Œä¸è¦æ··æ·†äº†ã€‚

#### å¯ä»¥é€šè¿‡ä¸€ç³»åˆ—çš„ç®—å­å¯¹rddè¿›è¡Œæ“ä½œï¼Œä¸»è¦åˆ†ä¸ºTransformationå’ŒActionä¸¤ç§æ“ä½œã€‚
1. Transformation(è½¬æ¢)ï¼šæ˜¯å¯¹å·²æœ‰çš„RDDè¿›è¡Œæ¢è¡Œç”Ÿæˆæ–°çš„RDDï¼Œå¯¹äºè½¬æ¢è¿‡ç¨‹é‡‡ç”¨æƒ°æ€§è®¡ç®—æœºåˆ¶ï¼Œä¸ä¼šç«‹å³è®¡ç®—å‡ºç»“æœã€‚å¸¸ç”¨çš„æ–¹æ³•æœ‰mapï¼Œfilterï¼Œflatmapç­‰ã€‚
2. Action(æ‰§è¡Œ)ï¼šå¯¹å·²æœ‰å¯¹RDDå¯¹æ•°æ®æ‰§è¡Œè®¡ç®—äº§ç”Ÿç»“æœï¼Œå¹¶å°†ç»“æœè¿”å›Driveræˆ–è€…å†™å…¥åˆ°å¤–éƒ¨å­˜å‚¨ä¸­ã€‚å¸¸ç”¨åˆ°æ–¹æ³•æœ‰reduceï¼Œcollectï¼ŒsaveAsTextFileç­‰ã€‚

#### RDDæ ¸å¿ƒå±æ€§
â¢ åˆ†åŒºåˆ—è¡¨
RDD æ•°æ®ç»“æ„ä¸­å­˜åœ¨åˆ†åŒºåˆ—è¡¨ï¼Œç”¨äºæ‰§è¡Œä»»åŠ¡æ—¶å¹¶è¡Œè®¡ç®—ï¼Œæ˜¯å®ç°åˆ†å¸ƒå¼è®¡ç®—çš„é‡è¦å±æ€§ã€‚
â¢ åˆ†åŒºè®¡ç®—å‡½æ•°
Spark åœ¨è®¡ç®—æ—¶ï¼Œæ˜¯ä½¿ç”¨åˆ†åŒºå‡½æ•°å¯¹æ¯ä¸€ä¸ªåˆ†åŒºè¿›è¡Œè®¡ç®—
â¢ RDD ä¹‹é—´çš„ä¾èµ–å…³ç³»
RDD æ˜¯è®¡ç®—æ¨¡å‹çš„å°è£…ï¼Œå½“éœ€æ±‚ä¸­éœ€è¦å°†å¤šä¸ªè®¡ç®—æ¨¡å‹è¿›è¡Œç»„åˆæ—¶ï¼Œå°±éœ€è¦å°†å¤šä¸ª RDD å»ºç«‹ä¾èµ–å…³ç³»
â¢ åˆ†åŒºå™¨ï¼ˆå¯é€‰ï¼‰
å½“æ•°æ®ä¸º KV ç±»å‹æ•°æ®æ—¶ï¼Œå¯ä»¥é€šè¿‡è®¾å®šåˆ†åŒºå™¨è‡ªå®šä¹‰æ•°æ®çš„åˆ†åŒº
â¢ é¦–é€‰ä½ç½®ï¼ˆå¯é€‰ï¼‰
è®¡ç®—æ•°æ®æ—¶ï¼Œå¯ä»¥æ ¹æ®è®¡ç®—èŠ‚ç‚¹çš„çŠ¶æ€é€‰æ‹©ä¸åŒçš„èŠ‚ç‚¹ä½ç½®è¿›è¡Œè®¡ç®—

#### åˆ›å»ºRDD
åœ¨ Spark ä¸­åˆ›å»º RDD çš„åˆ›å»ºæ–¹å¼å¯ä»¥åˆ†ä¸ºå››ç§ï¼š
1. ä»é›†åˆï¼ˆå†…å­˜ï¼‰ä¸­åˆ›å»º RDDï¼Œä»é›†åˆä¸­åˆ›å»º RDDï¼ŒSpark ä¸»è¦æä¾›äº†ä¸¤ä¸ªæ–¹æ³•ï¼šparallelize å’Œ makeRDDã€‚
	ä»åº•å±‚ä»£ç å®ç°æ¥è®²ï¼ŒmakeRDD æ–¹æ³•å…¶å®å°±æ˜¯ parallelize æ–¹æ³•  
```
val sparkConf =  new SparkConf().setMaster("local[*]").setAppName("spark")
val sparkContext = new SparkContext(sparkConf)
val rdd1 = sparkContext.parallelize(
 List(1,2,3,4)
)
val rdd2 = sparkContext.makeRDD(
 List(1,2,3,4)
)
rdd1.collect().foreach(println)
rdd2.collect().foreach(println)
sparkContext.stop()
```
2. ä»å¤–éƒ¨å­˜å‚¨ï¼ˆæ–‡ä»¶ï¼‰åˆ›å»º RDDï¼Œç”±å¤–éƒ¨å­˜å‚¨ç³»ç»Ÿçš„æ•°æ®é›†åˆ›å»º RDD åŒ…æ‹¬ï¼šæœ¬åœ°çš„æ–‡ä»¶ç³»ç»Ÿï¼Œæ‰€æœ‰ Hadoop æ”¯æŒçš„æ•°æ®é›†ï¼Œæ¯”å¦‚ HDFSã€HBase ç­‰ã€‚
3. ä»å…¶ä»– RDD åˆ›å»ºï¼Œä¸»è¦æ˜¯é€šè¿‡ä¸€ä¸ª RDD è¿ç®—å®Œåï¼Œå†äº§ç”Ÿæ–°çš„ RDDã€‚è¯¦æƒ…è¯·å‚è€ƒåç»­ç« èŠ‚ã€‚
4. ç›´æ¥åˆ›å»º RDDï¼ˆnewï¼‰ï¼Œä½¿ç”¨ new çš„æ–¹å¼ç›´æ¥æ„é€  RDDï¼Œä¸€èˆ¬ç”± Spark æ¡†æ¶è‡ªèº«ä½¿ç”¨ã€‚

#### RDDç®—å­ Operatorï¼Œåˆå«RDD æ–¹æ³•
1. è½¬æ¢ç®—å­ï¼šåŠŸèƒ½çš„è¡¥å……å’Œå°è£…ï¼Œå°†æ—§çš„RDDåŒ…è£…æˆæ–°çš„RDD
2. è¡ŒåŠ¨ç®—å­ï¼šè§¦å‘ä»»åŠ¡çš„è°ƒåº¦å’Œä½œä¸šçš„æ‰§è¡Œ  
[å‚è€ƒè¿™ç¯‡æ–‡ç« æ‰€åˆ—çš„è¯¦ç»†è¯´æ˜è®°å¿†](https://blog.csdn.net/weixin_44966780/article/details/122323347)  
*åç»­å†æŸ¥è¯¢ä¸€ä¸‹å¤§å…¨ï¼Œç®—å­éå¸¸é‡è¦éœ€è¦æ­»è®°ç¡¬èƒŒï¼Œå…ˆèƒŒå¸¸ç”¨ï¼Œé¢è¯•åº”è¯¥ä¸ä¼šå…¨éƒ¨é—®*

#### RDDåºåˆ—åŒ–
[å‚è€ƒæ–‡æ¡£â€”â€”â€”â€”SparkCoreä¹‹RDDåºåˆ—åŒ–](https://blog.csdn.net/weixin_42796403/article/details/111874542)  
[å‚è€ƒæ–‡æ¡£â€”â€”â€”â€”RDD åºåˆ—åŒ–](https://blog.csdn.net/To_9426464/article/details/113838897)  
åºåˆ—åŒ–çš„å®šä¹‰æ˜¯ï¼Œå°†ä¸€ä¸ªå¯¹è±¡ç¼–ç æˆä¸€ä¸ªå­—èŠ‚æµ(I/O); è€Œä¸ä¹‹ç›¸åçš„æ“ä½œè¢«ç§°ä¸ºååºåˆ—åŒ–ã€‚  
åœ¨SparkRDDç¼–ç¨‹ä¸­ï¼Œåˆå§‹åŒ–å·¥ä½œæ˜¯åœ¨Driverç«¯è¿›è¡Œçš„ï¼Œè€Œå®é™…è¿è¡Œç¨‹åºæ˜¯åœ¨Executorç«¯è¿›è¡Œçš„ï¼Œæ¶‰åŠåˆ°äº†è·¨è¿›ç¨‹é€šä¿¡ï¼Œæ˜¯éœ€è¦åºåˆ—åŒ–çš„ã€‚
```
RDDåºåˆ—åŒ–ï¼Œåœ¨ç±»çš„åºåˆ—åŒ–æŠ¥é”™æ—¶ï¼Œå³ç”¨åˆ°ç±»çš„å±æ€§æ–¹æ³•ç­‰ï¼Œæä¾›ä¸¤ç§æ€è·¯ï¼š
1. å°†ç±»åºåˆ—åŒ–ï¼Œæ¯”å¦‚æ··å…¥ç‰¹è´¨ï¼Œæˆ–å˜æˆæ ·ä¾‹ç±»case
2. å°†å±æ€§èµ‹å€¼ç»™æ–¹æ³•ä¸­çš„ä¸€ä¸ªä¸´æ—¶å˜é‡ï¼Œæ”¹å˜ç”Ÿå‘½å‘¨æœŸï¼Œå½¢æˆé—­åŒ…ï¼Œå³å°†è¿™ä¸ªå±æ€§ä¸ç±»å‰¥ç¦»
3. ä¸‰ä¸ªç¤ºä¾‹ä»£ç ï¼Œæ³¨æ„æŸ¥çœ‹æ³¨é‡Š

	// å®šä¹‰ç±»æ—¶æ··å…¥ç‰¹è´¨
	class User extends Serializable {
	    var age : Int = 30
	}

	// æ ·ä¾‹ç±»åœ¨ç¼–è¯‘æ—¶ï¼Œä¼šè‡ªåŠ¨æ··å…¥åºåˆ—åŒ–ç‰¹è´¨ï¼ˆå®ç°å¯åºåˆ—åŒ–æ¥å£ï¼‰
	case class User() {
	    var age : Int = 30
	}
	
	// ç±»çš„æ„é€ å‚æ•°å…¶å®æ˜¯ç±»çš„å±æ€§, æ„é€ å‚æ•°éœ€è¦è¿›è¡Œé—­åŒ…æ£€æµ‹ï¼Œå…¶å®å°±ç­‰åŒäºç±»è¿›è¡Œé—­åŒ…æ£€æµ‹
	class Search(query:String){

		def isMatch(s: String): Boolean = {
			s.contains(this.query)
		}

		// å‡½æ•°åºåˆ—åŒ–æ¡ˆä¾‹
		def getMatch1 (rdd: RDD[String]): RDD[String] = {
			rdd.filter(isMatch)
		}

		// å±æ€§åºåˆ—åŒ–æ¡ˆä¾‹
		def getMatch2(rdd: RDD[String]): RDD[String] = {
			/*
			å°†å±æ€§queryå˜æˆæ–¹æ³•çš„å±€éƒ¨å˜é‡ï¼Œä¸ç±»Searchå‰¥ç¦»ï¼Œå› ä¸ºå­—ç¬¦ä¸²æœ¬èº«æ˜¯å·²ç»å®ç°äº†Serializableæ¥å£ï¼Œå·²ç»æ˜¯åºåˆ—åŒ–äº†çš„
			*/
			val s = query 
			rdd.filter(x => x.contains(s))
		}
	}
```
**Kryo åºåˆ—åŒ–æ¡†æ¶**
```
Sparké»˜è®¤ä½¿ç”¨Javaçš„åºåˆ—åŒ–å™¨ï¼ŒJavaçš„åºåˆ—åŒ–èƒ½å¤Ÿåºåˆ—åŒ–ä»»ä½•çš„ç±»ã€‚ä½†æ˜¯æ¯”è¾ƒé‡ï¼Œåºåˆ—åŒ–åå¯¹è±¡çš„ä½“ç§¯ä¹Ÿæ¯”è¾ƒå¤§ã€‚
Sparkå‡ºäºæ€§èƒ½çš„è€ƒè™‘ï¼ŒSpark2.0å¼€å§‹æ”¯æŒå¦å¤–ä¸€ç§Kryoåºåˆ—åŒ–æœºåˆ¶ã€‚Kryoé€Ÿåº¦æ˜¯Serializableçš„10å€ã€‚å½“RDDåœ¨Shuffleæ•°æ®çš„æ—¶å€™ï¼Œç®€å•æ•°æ®ç±»å‹ã€æ•°ç»„å’Œå­—ç¬¦ä¸²ç±»å‹å·²ç»åœ¨Sparkå†…éƒ¨ä½¿ç”¨Kryoæ¥åºåˆ—åŒ–ã€‚
æ³¨æ„ï¼šå³ä½¿ä½¿ç”¨Kryoåºåˆ—åŒ–ï¼Œä¹Ÿè¦ç»§æ‰¿Serializableæ¥å£ã€‚
```

#### é—­åŒ…æ£€æµ‹
[å‚è€ƒæ–‡æ¡£â€”â€”â€”â€”Sparkä¸­çš„é—­åŒ…å’Œé—­åŒ…æ£€æµ‹](https://blog.51cto.com/u_12902538/3727054)  
é—­åŒ…æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œè¿”å›å€¼ä¾èµ–äºå£°æ˜åœ¨å‡½æ•°å¤–éƒ¨çš„ä¸€ä¸ªæˆ–å¤šä¸ªå˜é‡ã€‚  
é—­åŒ…é€šå¸¸æ¥è®²å¯ä»¥ç®€å•çš„è®¤ä¸ºæ˜¯å¯ä»¥è®¿é—®ä¸€ä¸ªå‡½æ•°é‡Œé¢å±€éƒ¨å˜é‡çš„å¦å¤–ä¸€ä¸ªå‡½æ•°ã€‚  
å‡½æ•°åœ¨å˜é‡ä¸å¤„äºå…¶æœ‰æ•ˆä½œç”¨åŸŸæ—¶ï¼Œè¿˜èƒ½å¤Ÿå¯¹å˜é‡è¿›è¡Œè®¿é—®ï¼Œå³ä¸ºé—­åŒ…ï¼›ä¹Ÿå°±æ˜¯è¯´ï¼Œå˜é‡è¶…å‡ºäº†å…¶ä½œç”¨åŸŸï¼Œè¿˜å¯ä»¥ä½¿ç”¨ï¼Œå°±æ˜¯é—­åŒ…ç°è±¡ã€‚  
`é—­åŒ… = ä»£ç  + ç”¨åˆ°çš„éå±€éƒ¨å˜é‡`
**ä»€ä¹ˆæ˜¯é—­åŒ…æ£€æµ‹ï¼Ÿ**
```
ä»è®¡ç®—çš„è§’åº¦, ç®—å­ä»¥å¤–çš„ä»£ç éƒ½æ˜¯åœ¨ Driver ç«¯æ‰§è¡Œ, ç®—å­é‡Œé¢çš„ä»£ç éƒ½æ˜¯åœ¨ Executorç«¯æ‰§è¡Œã€‚
é‚£ä¹ˆåœ¨ scala çš„å‡½æ•°å¼ç¼–ç¨‹ä¸­ï¼Œå°±ä¼šå¯¼è‡´ç®—å­å†…ç»å¸¸ä¼šç”¨åˆ°ç®—å­å¤–çš„æ•°æ®ï¼Œè¿™æ ·å°±å½¢æˆäº†é—­åŒ…çš„æ•ˆæœï¼Œ
å¦‚æœä½¿ç”¨çš„ç®—å­å¤–çš„æ•°æ®æ— æ³•åºåˆ—åŒ–ï¼Œå°±æ„å‘³ç€æ— æ³•ä¼ å€¼ç»™ Executorç«¯æ‰§è¡Œï¼Œå°±ä¼šå‘ç”Ÿé”™è¯¯ï¼Œ
æ‰€ä»¥éœ€è¦åœ¨æ‰§è¡Œä»»åŠ¡è®¡ç®—å‰ï¼Œæ£€æµ‹é—­åŒ…å†…çš„å¯¹è±¡æ˜¯å¦å¯ä»¥è¿›è¡Œåºåˆ—åŒ–ï¼Œè¿™ä¸ªæ“ä½œæˆ‘ä»¬ç§°ä¹‹ä¸ºé—­åŒ…æ£€æµ‹ã€‚
Scala2.12 ç‰ˆæœ¬åé—­åŒ…ç¼–è¯‘æ–¹å¼å‘ç”Ÿäº†æ”¹å˜
```

#### RDDå®½çª„ä¾èµ–
1. RDD è¡€ç¼˜å…³ç³»ï¼š
	å¤šä¸ª RDD ä¹‹é—´çš„ä¾èµ–å…³ç³»ç§°ä¸ºè¡€ç¼˜å…³ç³»ã€‚
	RDD åªæ”¯æŒç²—ç²’åº¦è½¬æ¢ï¼Œå³åœ¨å¤§é‡è®°å½•ä¸Šæ‰§è¡Œçš„å•ä¸ªæ“ä½œã€‚å°†åˆ›å»º RDD çš„ä¸€ç³»åˆ— Lineageï¼ˆè¡€ç»Ÿï¼‰è®°å½•ä¸‹æ¥ï¼Œä»¥ä¾¿æ¢å¤ä¸¢å¤±çš„åˆ†åŒºã€‚
	RDD çš„ Lineage ä¼šè®°å½• RDD çš„å…ƒæ•°æ®ä¿¡æ¯å’Œè½¬æ¢è¡Œä¸ºï¼Œå½“è¯¥ RDD çš„éƒ¨åˆ†åˆ†åŒºæ•°æ®ä¸¢å¤±æ—¶ï¼Œå®ƒå¯ä»¥æ ¹æ®è¿™äº›ä¿¡æ¯æ¥é‡æ–°è¿ç®—å’Œæ¢å¤ä¸¢å¤±çš„æ•°æ®åˆ†åŒºã€‚  
	RDD çš„è®¡ç®—ä¸­ä¸€æ—¦å‡ºé”™ï¼Œå¯ä»¥é€šè¿‡è¡€ç¼˜å…³ç³»å°†æ•°æ®é‡æ–°è¯»å–è¿›è¡Œè®¡ç®—ã€‚  
	`.toDebugString` æ‰“å°è¡€ç¼˜å…³ç³»  
2. RDD ä¾èµ–å…³ç³»ï¼š
	è¿™é‡Œæ‰€è°“çš„ä¾èµ–å…³ç³»ï¼Œå…¶å®å°±æ˜¯ä¸¤ä¸ªç›¸é‚» RDD ä¹‹é—´çš„å…³ç³»ã€‚*æ³¨æ„* å¤šä¸ª RDD ä¹‹é—´çš„ä¾èµ–å…³ç³»ç§°ä¸ºè¡€ç¼˜å…³ç³»ã€‚  
	`.dependencies` æ‰“å°ä¾èµ–å…³ç³»  
3. RDD çª„ä¾èµ–ï¼š
	çª„ä¾èµ–è¡¨ç¤ºæ¯ä¸€ä¸ªçˆ¶(ä¸Šæ¸¸)RDD çš„ Partition æœ€å¤šè¢«å­ï¼ˆä¸‹æ¸¸ï¼‰RDD çš„ä¸€ä¸ª Partition ä½¿ç”¨ï¼Œçª„ä¾èµ–æˆ‘ä»¬å½¢è±¡çš„æ¯”å–»ä¸ºç‹¬ç”Ÿå­å¥³ã€‚
4. RDD å®½ä¾èµ–ï¼š
	å®½ä¾èµ–è¡¨ç¤ºåŒä¸€ä¸ªçˆ¶ï¼ˆä¸Šæ¸¸ï¼‰RDD çš„ Partition è¢«å¤šä¸ªå­ï¼ˆä¸‹æ¸¸ï¼‰RDD çš„ Partition ä¾èµ–ï¼Œä¼šå¼•èµ· Shuffleï¼Œæ€»ç»“ï¼šå®½ä¾èµ–æˆ‘ä»¬å½¢è±¡çš„æ¯”å–»ä¸ºå¤šç”Ÿã€‚
5. RDD é˜¶æ®µåˆ’åˆ†ï¼š
	DAGï¼ˆDirected Acyclic Graphï¼‰æœ‰å‘æ— ç¯å›¾æ˜¯ç”±ç‚¹å’Œçº¿ç»„æˆçš„æ‹“æ‰‘å›¾å½¢ï¼Œè¯¥å›¾å½¢å…·æœ‰æ–¹å‘ï¼Œä¸ä¼šé—­ç¯ã€‚ä¾‹å¦‚ï¼ŒDAG è®°å½•äº† RDD çš„è½¬æ¢è¿‡ç¨‹å’Œä»»åŠ¡çš„é˜¶æ®µã€‚
	*æ³¨æ„ï¼ï¼ï¼è¿™é‡Œå¯ä»¥å‚è€ƒä¸€ä¸‹æºç ä¸­çš„è§£é‡Š*

#### RDD -> DAG
DAGæ˜¯ä¸€ä¸ªæœ‰å‘æ— ç¯å›¾ï¼Œåœ¨Sparkä¸­ï¼Œ ä½¿ç”¨ DAG æ¥æè¿°æˆ‘ä»¬çš„è®¡ç®—é€»è¾‘ã€‚ä¸»è¦åˆ†ä¸ºDAG Scheduler å’ŒTask Schedulerã€‚
1. DAG Scheduler
```
DAG Scheduler æ˜¯é¢å‘stageçš„é«˜å±‚çº§çš„è°ƒåº¦å™¨ï¼ŒDAG ScheduleræŠŠDAGæ‹†åˆ†ä¸ºå¤šä¸ªTaskï¼Œæ¯ç»„Taskéƒ½æ˜¯ä¸€ä¸ªstageï¼Œ
è§£ææ—¶æ˜¯ä»¥shuffleä¸ºè¾¹ç•Œè¿›è¡Œåå‘æ„å»ºçš„ï¼Œæ¯å½“é‡è§ä¸€ä¸ªshuffleï¼Œsparkå°±ä¼šäº§ç”Ÿä¸€ä¸ªæ–°çš„stageï¼Œ
æ¥ç€ä»¥TaskSetçš„å½¢å¼æäº¤ç»™åº•å±‚çš„è°ƒåº¦å™¨ï¼ˆtask schedulerï¼‰ï¼Œæ¯ä¸ªstageå°è£…æˆä¸€ä¸ªTaskSetã€‚
DAG Scheduleréœ€è¦è®°å½•RDDè¢«å­˜å…¥ç£ç›˜ç‰©åŒ–ç­‰åŠ¨ä½œï¼ŒåŒæ—¶ä¼šéœ€è¦Taskå¯»æ‰¾æœ€ä¼˜ç­‰è°ƒåº¦é€»è¾‘ï¼Œä»¥åŠç›‘æ§å› shuffleè·¨èŠ‚ç‚¹è¾“å‡ºå¯¼è‡´çš„å¤±è´¥ã€‚
```
2. Task Scheduler
```
Task Scheduler è´Ÿè´£æ¯ä¸€ä¸ªå…·ä½“ä»»åŠ¡çš„æ‰§è¡Œã€‚å®ƒçš„ä¸»è¦èŒè´£åŒ…æ‹¬
ä»»åŠ¡é›†çš„è°ƒåº¦ç®¡ç†
çŠ¶æ€ç»“æœè·Ÿè¸ª
ç‰©ç†èµ„æºè°ƒåº¦ç®¡ç†
ä»»åŠ¡æ‰§è¡Œ
è·å–ç»“æœ
```

#### RDDä»»åŠ¡åˆ’åˆ†
RDD ä»»åŠ¡åˆ‡åˆ†ä¸­é—´åˆ†ä¸ºï¼šApplicationã€Jobã€Stage å’Œ Task
1. Applicationï¼šåˆå§‹åŒ–ä¸€ä¸ª SparkContext å³ç”Ÿæˆä¸€ä¸ª Applicationï¼›
2. Jobï¼šä¸€ä¸ª Action ç®—å­å°±ä¼šç”Ÿæˆä¸€ä¸ª Jobï¼›
3. Stageï¼šStage ç­‰äºå®½ä¾èµ–(ShuffleDependency)çš„ä¸ªæ•°åŠ  1ï¼›
4. Taskï¼šä¸€ä¸ª Stage é˜¶æ®µä¸­ï¼Œæœ€åä¸€ä¸ª RDD çš„åˆ†åŒºä¸ªæ•°å°±æ˜¯ Task çš„ä¸ªæ•°ã€‚
**æ³¨æ„ï¼šApplication->Job->Stage->Task æ¯ä¸€å±‚éƒ½æ˜¯ 1 å¯¹ n çš„å…³ç³»ã€‚**
*æºç å…³é”®è¯ï¼š`partitionsToCompute ShuffleMapStage ShuffleMapTask ResultStage ResultTask`*  

##### RDDä»»åŠ¡åˆ’åˆ†â€”â€”â€”â€”Job
jobæ˜¯æœ‰å¤šä¸ªstageæ„å»ºçš„å¹¶è¡Œçš„è®¡ç®—ä»»åŠ¡ï¼Œjobæ˜¯ç”±sparkçš„actionæ“ä½œæ¥è§¦å‘çš„ï¼Œåœ¨sparkä¸­ä¸€ä¸ªjobåŒ…å«å¤šä¸ªRDDä»¥åŠä½œç”¨åœ¨RDDçš„å„ç§æ“ä½œç®—å­ã€‚

##### RDDä»»åŠ¡åˆ’åˆ†â€”â€”â€”â€”Stage
DAG Schedulerä¼šæŠŠDAGåˆ‡å‰²æˆå¤šä¸ªç›¸äº’ä¾èµ–çš„Stageï¼Œåˆ’åˆ†Stageçš„ä¸€ä¸ªä¾æ®æ˜¯RDDé—´çš„å®½çª„ä¾èµ–ã€‚
åœ¨å¯¹Jobä¸­çš„æ‰€æœ‰æ“ä½œåˆ’åˆ†Stageæ—¶ï¼Œä¸€èˆ¬ä¼šæŒ‰ç…§å€’åºè¿›è¡Œï¼Œå³ä»Actionå¼€å§‹ï¼Œé‡åˆ°çª„ä¾èµ–æ“ä½œï¼Œåˆ™åˆ’åˆ†åˆ°åŒä¸€ä¸ªæ‰§è¡Œé˜¶æ®µï¼Œé‡åˆ°å®½ä¾èµ–æ“ä½œï¼Œ
åˆ™åˆ’åˆ†ä¸€ä¸ªæ–°çš„æ‰§è¡Œé˜¶æ®µï¼Œä¸”æ–°çš„é˜¶æ®µä¸ºä¹‹å‰é˜¶æ®µçš„parentï¼Œç„¶åä¾æ¬¡ç±»æ¨é€’å½’æ‰§è¡Œã€‚
child Stageéœ€è¦ç­‰å¾…æ‰€æœ‰çš„parent Stageæ‰§è¡Œå®Œä¹‹åæ‰å¯ä»¥æ‰§è¡Œï¼Œè¿™æ—¶Stageä¹‹é—´æ ¹æ®ä¾èµ–å…³ç³»æ„æˆäº†ä¸€ä¸ªå¤§ç²’åº¦çš„DAGã€‚
åœ¨ä¸€ä¸ªStageå†…ï¼Œæ‰€æœ‰çš„æ“ä½œä»¥ä¸²è¡Œçš„Pipelineçš„æ–¹å¼ï¼Œç”±ä¸€ç»„Taskå®Œæˆè®¡ç®—ã€‚

##### RDDä»»åŠ¡åˆ’åˆ†â€”â€”â€”â€”TaskSet Task
TaskSet å¯ä»¥ç†è§£ä¸ºä¸€ç§ä»»åŠ¡ï¼Œå¯¹åº”ä¸€ä¸ªstageï¼Œæ˜¯Taskç»„æˆçš„ä»»åŠ¡é›†ã€‚ä¸€ä¸ªTaskSetä¸­çš„æ‰€æœ‰Taskæ²¡æœ‰shuffleä¾èµ–å¯ä»¥å¹¶è¡Œè®¡ç®—ã€‚
Taskæ˜¯sparkä¸­æœ€ç‹¬ç«‹çš„è®¡ç®—å•å…ƒï¼Œç”±Driver Managerå‘é€åˆ°executeræ‰§è¡Œï¼Œé€šå¸¸æƒ…å†µä¸€ä¸ªtaskå¤„ç†spark RDDä¸€ä¸ªpartitionã€‚
Taskåˆ†ä¸ºShuffleMapTaskå’ŒResultTaskä¸¤ç§ï¼Œä½äºæœ€åä¸€ä¸ªStageçš„Taskä¸ºResultTaskï¼Œå…¶ä»–é˜¶æ®µçš„å±äºShuffleMapTaskã€‚

#### RDDæŒä¹…åŒ–
[å‚è€ƒæ–‡æ¡£â€”â€”â€”â€”sparkæŒä¹…åŒ–æ“ä½œ persist(),cache()](https://blog.csdn.net/donger__chen/article/details/86366339)  
[å‚è€ƒæ–‡æ¡£â€”â€”â€”â€”Spark æŒä¹…åŒ–ï¼ˆcacheå’Œpersistçš„åŒºåˆ«ï¼‰](https://blog.csdn.net/dkl12/article/details/80742498/)  
RDD ä¸­ä¸å­˜å‚¨æ•°æ®ï¼Œå¦‚æœä¸€ä¸ª RDD éœ€è¦é‡å¤ä½¿ç”¨ï¼Œé‚£ä¹ˆéœ€è¦é‡å¤´å†æ¥å†æ¬¡æ‰§è¡Œæ¥è·å–æ•°æ®ï¼ŒRDD å¯¹è±¡å¯ä»¥é‡ç”¨ï¼Œä½†æ˜¯æ•°æ®æ— æ³•é‡ç”¨ï¼Œé‡ç”¨å¯¹è±¡ä¾æ—§ä¼šé‡å¤ä¹‹å‰çš„è®¡ç®—è¿‡ç¨‹  
è¿™æ ·å°±éœ€è¦ç¼“å­˜ RDD ï¼Œè¿™æ ·æ•°æ®å°±å¯ä»¥é‡å¤ä½¿ç”¨  
1. RDD Cache ç¼“å­˜
```
RDD é€šè¿‡ Cache æˆ–è€… Persist æ–¹æ³•å°†å‰é¢çš„è®¡ç®—ç»“æœç¼“å­˜ï¼Œé»˜è®¤æƒ…å†µä¸‹ä¼šæŠŠæ•°æ®ä»¥ç¼“å­˜åœ¨ JVM çš„å †å†…å­˜ä¸­ã€‚
ä½†æ˜¯å¹¶ä¸æ˜¯è¿™ä¸¤ä¸ªæ–¹æ³•è¢«è°ƒç”¨æ—¶ç«‹å³ç¼“å­˜ï¼Œè€Œæ˜¯è§¦å‘åé¢çš„ action ç®—å­æ—¶ï¼Œè¯¥ RDD å°†ä¼šè¢«ç¼“å­˜åœ¨è®¡ç®—èŠ‚ç‚¹çš„å†…å­˜ä¸­ï¼Œå¹¶ä¾›åé¢é‡ç”¨ã€‚

ç¼“å­˜æœ‰å¯èƒ½ä¸¢å¤±ï¼Œæˆ–è€…å­˜å‚¨äºå†…å­˜çš„æ•°æ®ç”±äºå†…å­˜ä¸è¶³è€Œè¢«åˆ é™¤ï¼ŒRDD çš„ç¼“å­˜å®¹é”™æœºåˆ¶ä¿è¯äº†å³ä½¿ç¼“å­˜ä¸¢å¤±ä¹Ÿèƒ½ä¿è¯è®¡ç®—çš„æ­£ç¡®æ‰§è¡Œã€‚
é€šè¿‡åŸºäº RDD çš„ä¸€ç³»åˆ—è½¬æ¢ï¼Œä¸¢å¤±çš„æ•°æ®ä¼šè¢«é‡ç®—ï¼Œç”±äº RDD çš„å„ä¸ª Partition æ˜¯ç›¸å¯¹ç‹¬ç«‹çš„ï¼Œå› æ­¤åªéœ€è¦è®¡ç®—ä¸¢å¤±çš„éƒ¨åˆ†å³å¯ï¼Œå¹¶ä¸éœ€è¦é‡ç®—å…¨éƒ¨ Partitionã€‚

Spark ä¼šè‡ªåŠ¨å¯¹ä¸€äº› Shuffle æ“ä½œçš„ä¸­é—´æ•°æ®åšæŒä¹…åŒ–æ“ä½œ(æ¯”å¦‚ï¼šreduceByKey)ã€‚
è¿™æ ·åšçš„ç›®çš„æ˜¯ä¸ºäº†å½“ä¸€ä¸ªèŠ‚ç‚¹ Shuffle å¤±è´¥äº†é¿å…é‡æ–°è®¡ç®—æ•´ä¸ªè¾“å…¥ã€‚ä½†æ˜¯ï¼Œåœ¨å®é™…ä½¿ç”¨çš„æ—¶å€™ï¼Œå¦‚æœæƒ³é‡ç”¨æ•°æ®ï¼Œä»ç„¶å»ºè®®è°ƒç”¨ persist æˆ– cacheã€‚
```
2. RDD CheckPoint æ£€æŸ¥ç‚¹
```
æ‰€è°“çš„æ£€æŸ¥ç‚¹å…¶å®å°±æ˜¯é€šè¿‡å°† RDD ä¸­é—´ç»“æœå†™å…¥ç£ç›˜

ç”±äºè¡€ç¼˜ä¾èµ–è¿‡é•¿ä¼šé€ æˆå®¹é”™æˆæœ¬è¿‡é«˜ï¼Œè¿™æ ·å°±ä¸å¦‚åœ¨ä¸­é—´é˜¶æ®µåšæ£€æŸ¥ç‚¹å®¹é”™ï¼Œå¦‚æœæ£€æŸ¥ç‚¹ä¹‹åæœ‰èŠ‚ç‚¹å‡ºç°é—®é¢˜ï¼Œå¯ä»¥ä»æ£€æŸ¥ç‚¹å¼€å§‹é‡åšè¡€ç¼˜ï¼Œå‡å°‘äº†å¼€é”€ã€‚

å¯¹ RDD è¿›è¡Œ checkpoint æ“ä½œå¹¶ä¸ä¼šé©¬ä¸Šè¢«æ‰§è¡Œï¼Œå¿…é¡»æ‰§è¡Œ Action æ“ä½œæ‰èƒ½è§¦å‘ã€‚
```
3. ç¼“å­˜ å’Œ æ£€æŸ¥ç‚¹ çš„åŒºåˆ«
	+ Cache ç¼“å­˜åªæ˜¯å°†æ•°æ®ä¿å­˜èµ·æ¥ï¼Œä¸åˆ‡æ–­è¡€ç¼˜ä¾èµ–ã€‚Checkpoint æ£€æŸ¥ç‚¹åˆ‡æ–­è¡€ç¼˜ä¾èµ–ã€‚
	+ Cache ç¼“å­˜çš„æ•°æ®é€šå¸¸å­˜å‚¨åœ¨ç£ç›˜ã€å†…å­˜ç­‰åœ°æ–¹ï¼Œä¸å®‰å…¨ä¸”å¯é æ€§ä½ã€‚Checkpoint çš„æ•°æ®é€šå¸¸å­˜å‚¨åœ¨ HDFS ç­‰å®¹é”™ã€é«˜å¯ç”¨çš„æ–‡ä»¶ç³»ç»Ÿï¼Œå¯é æ€§é«˜ã€‚
	+ å»ºè®®å¯¹ checkpoint()çš„ RDD ä½¿ç”¨ Cache ç¼“å­˜ï¼Œè¿™æ · checkpoint çš„ job åªéœ€ä» Cache ç¼“å­˜ä¸­è¯»å–æ•°æ®å³å¯ï¼Œå¦åˆ™éœ€è¦å†ä»å¤´è®¡ç®—ä¸€æ¬¡ RDDã€‚
4. cacheï¼Œpersistï¼Œcheckpointçš„åŒºåˆ«
	+ cache å°†æ•°æ®*ä¸´æ—¶*å­˜å‚¨åœ¨å†…å­˜ä¸­è¿›è¡Œæ•°æ®é‡ç”¨ï¼Œä¼šæ·»åŠ æ–°çš„è¡€ç¼˜å…³ç³»ä¾èµ–ï¼Œä¸€æ—¦å‡ºç°é—®é¢˜å¯ä»¥é‡è¯»æ•°æ®  
	+ persist å°†æ•°æ®*ä¸´æ—¶*å­˜å‚¨åœ¨ç£ç›˜æ–‡ä»¶ä¸­è¿›è¡Œæ•°æ®é‡ç”¨ï¼Œ*æ¶‰åŠåˆ°ç£ç›˜IOï¼Œæ€§èƒ½è¾ƒä½ï¼Œä½†æ˜¯æ•°æ®å®‰å…¨ï¼Œå¦‚æœä½œä¸šæ‰§è¡Œå®Œæ¯•ï¼Œä¸´æ—¶ä¿å­˜çš„æ•°æ®æ–‡ä»¶å°±ä¼šä¸¢å¤±*
	+ checkpoint å°†æ•°æ®*æ°¸ä¹…*å­˜å‚¨åœ¨ç£ç›˜æ–‡ä»¶ä¸­è¿›è¡Œæ•°æ®é‡ç”¨ï¼Œ*æ¶‰åŠåˆ°ç£ç›˜IOï¼Œæ€§èƒ½è¾ƒä½ï¼Œä½†æ˜¯æ•°æ®å®‰å…¨ï¼Œä¸€èˆ¬ä¼šç‹¬ç«‹æ‰§è¡Œä½œä¸šï¼Œä¼šé™ä½æ•ˆç‡*ï¼Œæ‰€ä»¥ä¸€èˆ¬å’Œcacheç»„åˆä½¿ç”¨ï¼Œ
		ä¼šåˆ‡æ–­è¡€ç¼˜å…³ç³»ï¼Œé‡æ–°å»ºç«‹è¡€ç¼˜å…³ç³»ï¼Œç›¸å½“äºé‡æ–°æ”¹å˜æ•°æ®æº  
5. æ³¨æ„ï¼ï¼ï¼cache()å’Œpersist()çš„ä½¿ç”¨æ˜¯æœ‰è§„åˆ™çš„ï¼š
```
å¿…é¡»åœ¨textfileè¯»å–æ•°æ®æˆ–transformç­‰åˆ›å»ºä¸€ä¸ªrddä¹‹åï¼Œç›´æ¥è¿ç»­è°ƒç”¨cache()æˆ–è€…persist()æ‰å¯ä»¥ï¼Œ
å¦‚æœå…ˆåˆ›å»ºä¸€ä¸ªrddï¼Œå†å•ç‹¬å¦èµ·ä¸€è¡Œæ‰§è¡Œcache()æˆ–è€…persist()ï¼Œæ˜¯æ²¡æœ‰ç”¨çš„ï¼Œè€Œä¸”ä¼šæŠ¥é”™ï¼Œå¤§é‡çš„æ–‡ä»¶ä¼šä¸¢å¤±ã€‚
```

#### RDD åˆ†åŒºå™¨
Spark ç›®å‰æ”¯æŒ Hash åˆ†åŒºå’Œ Range åˆ†åŒºï¼Œå’Œç”¨æˆ·è‡ªå®šä¹‰åˆ†åŒºã€‚Hash åˆ†åŒºä¸ºå½“å‰çš„é»˜è®¤åˆ†åŒºã€‚
åˆ†åŒºå™¨ç›´æ¥å†³å®šäº† RDD ä¸­åˆ†åŒºçš„ä¸ªæ•°ã€RDD ä¸­æ¯æ¡æ•°æ®ç»è¿‡ Shuffle åè¿›å…¥å“ªä¸ªåˆ†åŒºï¼Œè¿›è€Œå†³å®šäº† Reduce çš„ä¸ªæ•°ã€‚
åªæœ‰ Key-Value ç±»å‹çš„ RDD æ‰æœ‰åˆ†åŒºå™¨ï¼Œé Key-Value ç±»å‹çš„ RDD åˆ†åŒºçš„å€¼æ˜¯ None
æ¯ä¸ª RDD çš„åˆ†åŒº ID èŒƒå›´ï¼š0 ~ (numPartitions - 1)ï¼Œå†³å®šè¿™ä¸ªå€¼æ˜¯å±äºé‚£ä¸ªåˆ†åŒºçš„ã€‚
1. Hash åˆ†åŒºï¼šå¯¹äºç»™å®šçš„ keyï¼Œè®¡ç®—å…¶ hashCode,å¹¶é™¤ä»¥åˆ†åŒºä¸ªæ•°å–ä½™
2. Range åˆ†åŒºï¼šå°†ä¸€å®šèŒƒå›´å†…çš„æ•°æ®æ˜ å°„åˆ°ä¸€ä¸ªåˆ†åŒºä¸­ï¼Œå°½é‡ä¿è¯æ¯ä¸ªåˆ†åŒºæ•°æ®å‡åŒ€ï¼Œè€Œä¸”åˆ†åŒºé—´æœ‰åº
**è‡ªå®šä¹‰åˆ†åŒºå™¨**
```
class partitionTest(Partitions:Int) extends Partitioner {
	override def numPartitions: Int = Partitions
	override def getPartition(key: Any):Int = {
		val a = if (key.toString.indexOf("hbase") != -1) {
			1
		}else if (key.toString.indexOf("spark") != -1){
			2
		}else{
			0
		}
	}
}
```

#### RDD æ–‡ä»¶è¯»å–ä¸ä¿å­˜
Spark çš„æ•°æ®è¯»å–åŠæ•°æ®ä¿å­˜å¯ä»¥ä»ä¸¤ä¸ªç»´åº¦æ¥ä½œåŒºåˆ†ï¼šæ–‡ä»¶æ ¼å¼ä»¥åŠæ–‡ä»¶ç³»ç»Ÿã€‚
æ–‡ä»¶æ ¼å¼åˆ†ä¸ºï¼štext æ–‡ä»¶ã€csv æ–‡ä»¶ã€sequence æ–‡ä»¶ä»¥åŠ Object æ–‡ä»¶ï¼›
æ–‡ä»¶ç³»ç»Ÿåˆ†ä¸ºï¼šæœ¬åœ°æ–‡ä»¶ç³»ç»Ÿã€HDFSã€HBASE ä»¥åŠæ•°æ®åº“ã€‚
â¢ text æ–‡ä»¶
â¢ sequence æ–‡ä»¶
SequenceFile æ–‡ä»¶æ˜¯ Hadoop ç”¨æ¥å­˜å‚¨äºŒè¿›åˆ¶å½¢å¼çš„ key-value å¯¹è€Œè®¾è®¡çš„ä¸€ç§å¹³é¢æ–‡ä»¶(Flat File)ã€‚åœ¨ SparkContext ä¸­ï¼Œå¯ä»¥è°ƒç”¨ sequenceFile[keyClass, valueClass](path)ã€‚
â¢ object å¯¹è±¡æ–‡ä»¶
å¯¹è±¡æ–‡ä»¶æ˜¯å°†å¯¹è±¡åºåˆ—åŒ–åä¿å­˜çš„æ–‡ä»¶ï¼Œé‡‡ç”¨ Java çš„åºåˆ—åŒ–æœºåˆ¶ã€‚å¯ä»¥é€šè¿‡ objectFile[T: ClassTag](path)å‡½æ•°æ¥æ”¶ä¸€ä¸ªè·¯å¾„ï¼Œè¯»å–å¯¹è±¡æ–‡ä»¶ï¼Œè¿”å›å¯¹åº”çš„ RDDï¼Œä¹Ÿå¯ä»¥é€šè¿‡è°ƒç”¨saveAsObjectFile()å®ç°å¯¹å¯¹è±¡æ–‡ä»¶çš„è¾“å‡ºã€‚å› ä¸ºæ˜¯åºåˆ—åŒ–æ‰€ä»¥è¦æŒ‡å®šç±»å‹ã€‚

### æ•°æ®ç»“æ„â€”â€”â€”â€”ç´¯åŠ å™¨(Acc)
#### ç´¯åŠ å™¨å®šä¹‰
åˆ†å¸ƒå¼å…±äº«åªå†™å˜é‡ï¼ˆExecutorç«¯çš„taskä¸èƒ½äº’ç›¸è®¿é—®ç´¯åŠ å™¨çš„å€¼ï¼‰ï¼Œç´¯åŠ å™¨ç”¨æ¥æŠŠ Executor ç«¯å˜é‡ä¿¡æ¯èšåˆåˆ° Driver ç«¯ã€‚

#### ç´¯åŠ å™¨åŸç†
Driver ç¨‹åºä¸­å®šä¹‰çš„å˜é‡ï¼Œåœ¨ Executor ç«¯çš„æ¯ä¸ª Task éƒ½ä¼šå¾—åˆ°è¿™ä¸ªå˜é‡çš„ä¸€ä»½æ–°çš„å‰¯æœ¬ï¼Œæ¯ä¸ª Task æ›´æ–°è¿™äº›å‰¯æœ¬çš„å€¼åï¼Œä¼ å› Driver ç«¯è¿›è¡Œ Mergeã€‚

#### è·å–ç´¯åŠ å™¨çš„å€¼
1. å°‘åŠ ï¼šè½¬æ¢ç®—å­ä¸­è°ƒç”¨ç´¯åŠ å™¨ï¼Œå¦‚æœæ²¡æœ‰è¡ŒåŠ¨ç®—å­ï¼Œé‚£ä¹ˆä¸ä¼šæ‰§è¡Œ  
2. å¤šåŠ ï¼šè½¬æ¢ç®—å­ä¸­è°ƒç”¨ç´¯åŠ å™¨ï¼Œå¦‚æœæ²¡æœ‰è¡ŒåŠ¨ç®—å­ï¼Œé‚£ä¹ˆä¸ä¼šæ‰§è¡Œ  
3. æ‰€ä»¥ä¸€èˆ¬æ”¾ç½®åœ¨è¡ŒåŠ¨ç®—å­ä¸­  

#### ç´¯åŠ å™¨ç¤ºä¾‹ä»£ç 
*ç³»ç»Ÿè‡ªå¸¦éƒ¨åˆ†ç´¯åŠ å™¨ï¼šlongAccumulator , doubleAccumulator , coolectionAccumulator*
```
Object Spark06_Accumulator {
  def main(args: Array[String]): Unit = {
    val conf: SparkConf = new SparkConf().setAppName(this.getClass.getName).setMaster("local[*]")
    val sc = new SparkContext(conf)
    val rdd: RDD[(String, Int)] = sc.makeRDD(List(("a", 1), ("b", 2), ("a", 3), ("b", 4)))
    // å£°æ˜ç´¯åŠ å™¨
    val sumAcc: LongAccumulator = sc.longAccumulator("sumAcc")
    rdd.foreach {
      case (word, count) => {
        // ä½¿ç”¨ç´¯åŠ å™¨
        sumAcc.add(count)
      }
    }
    // ç´¯åŠ å™¨çš„toStringæ–¹æ³•
    //println(sumAcc)
    //å–å‡ºç´¯åŠ å™¨ä¸­çš„å€¼
    println(sumAcc.value)
    sc.stop()
  }
}

```
#### è‡ªå®šä¹‰ç´¯åŠ å™¨ç¤ºä¾‹ä»£ç 
```
object Spark07_MyAccumulator {
  def main(args: Array[String]): Unit = {
    val conf: SparkConf = new SparkConf().setAppName(this.getClass.getName).setMaster("local[*]")
    val sc = new SparkContext(conf)
    val rdd: RDD[String] = sc.makeRDD(List("Hello", "HaHa", "spark", "scala", "Hi", "Hello", "Hi"))
    // åˆ›å»ºç´¯åŠ å™¨
    val myAcc = new MyAccumulator
    //æ³¨å†Œç´¯åŠ å™¨
    sc.register(myAcc, "MyAcc")
    rdd.foreach{
      datas => {
        // ä½¿ç”¨ç´¯åŠ å™¨
        myAcc.add(datas)
      }
    }
    // è·å–ç´¯åŠ å™¨çš„ç»“æœ
    println(myAcc.value)

    sc.stop()
  }
}

// è‡ªå®šä¹‰ç´¯åŠ å™¨
// æ³›å‹åˆ†åˆ«ä¸ºè¾“å…¥ç±»å‹å’Œè¾“å‡ºç±»å‹
class MyAccumulator extends AccumulatorV2[String, mutable.Map[String, Int]] {
  // å®šä¹‰è¾“å‡ºæ•°æ®å˜é‡
  var map: mutable.Map[String, Int] = mutable.Map[String, Int]()

  // ç´¯åŠ å™¨æ˜¯å¦ä¸ºåˆå§‹çŠ¶æ€
  override def isZero: Boolean = map.isEmpty

  // å¤åˆ¶ç´¯åŠ å™¨
  override def copy(): AccumulatorV2[String, mutable.Map[String, Int]] = {
    val MyAcc = new MyAccumulator
    // å°†æ­¤ç´¯åŠ å™¨ä¸­çš„æ•°æ®èµ‹å€¼ç»™æ–°åˆ›å»ºçš„ç´¯åŠ å™¨
    MyAcc.map = this.map
    MyAcc
  }

  // é‡ç½®ç´¯åŠ å™¨
  override def reset(): Unit = {
    map.clear()
  }

  // ç´¯åŠ å™¨æ·»åŠ å…ƒç´ 
  override def add(v: String): Unit = {
    if (v.startsWith("H")) {
      // åˆ¤æ–­mapé›†åˆä¸­æ˜¯å¦å·²ç»å­˜åœ¨æ­¤å…ƒç´ 
      map(v) = map.getOrElse(v, 0) + 1
    }
  }

  // åˆå¹¶ç´¯åŠ å™¨ä¸­çš„å…ƒç´ 
  override def merge(other: AccumulatorV2[String, mutable.Map[String, Int]]): Unit = {
    val map1: mutable.Map[String, Int] = this.map
    val map2: mutable.Map[String, Int] = other.value
    // åˆå¹¶ä¸¤ä¸ªmap
    map = map1.foldLeft(map2) {
      (m, kv) => {
        m(kv._1) = m.getOrElse(kv._1, 0) + kv._2
        m
      }
    }
  }

  // è·å–ç´¯åŠ å™¨ä¸­çš„å€¼
  override def value: mutable.Map[String, Int] = {
    map
  }
}

```


### æ•°æ®ç»“æ„â€”â€”â€”â€”å¹¿æ’­å˜é‡
#### å¹¿æ’­å˜é‡å®šä¹‰
1. å¹¿æ’­å˜é‡æ˜¯ä¸€ä¸ªåªè¯»å˜é‡

2. é€šè¿‡å®ƒæˆ‘ä»¬å¯ä»¥å°†ä¸€äº›å…±äº«æ•°æ®é›†æˆ–è€…å¤§å˜é‡ç¼“å­˜åœ¨Sparké›†ç¾¤ä¸­çš„å„ä¸ªæœºå™¨ä¸Šè€Œä¸ç”¨æ¯ä¸ªtaskéƒ½éœ€è¦copyä¸€ä¸ªå‰¯æœ¬ï¼Œ
	åç»­è®¡ç®—å¯ä»¥é‡å¤ä½¿ç”¨ï¼Œå‡å°‘äº†æ•°æ®ä¼ è¾“æ—¶ç½‘ç»œå¸¦å®½çš„ä½¿ç”¨ï¼Œæé«˜æ•ˆç‡ã€‚ç›¸æ¯”äºHadoopçš„åˆ†å¸ƒå¼ç¼“å­˜ï¼Œå¹¿æ’­çš„å†…å®¹å¯ä»¥è·¨ä½œä¸šå…±äº«ã€‚

3. å¹¿æ’­å˜é‡è¦æ±‚å¹¿æ’­çš„æ•°æ®ä¸å¯å˜ã€ä¸èƒ½å¤ªå¤§ä½†ä¹Ÿä¸èƒ½å¤ªå°(ä¸€èˆ¬å‡ åMä»¥ä¸Š)ã€å¯è¢«åºåˆ—åŒ–å’Œååºåˆ—åŒ–ã€å¹¶ä¸”å¿…é¡»åœ¨driverç«¯å£°æ˜å¹¿æ’­å˜é‡ï¼Œ
	é€‚ç”¨äºå¹¿æ’­å¤šä¸ªstageå…¬ç”¨çš„æ•°æ®ï¼Œå­˜å‚¨çº§åˆ«ç›®å‰æ˜¯MEMORY_AND_DISKã€‚

4. å¹¿æ’­å˜é‡å­˜å‚¨ç›®å‰åŸºäºSparkå®ç°çš„BlockManageråˆ†å¸ƒå¼å­˜å‚¨ç³»ç»Ÿï¼ŒSparkä¸­çš„shuffleæ•°æ®ã€åŠ è½½HDFSæ•°æ®æ—¶åˆ‡åˆ†è¿‡æ¥çš„blockå—éƒ½å­˜å‚¨åœ¨BlockManagerä¸­ï¼Œ
	ä¸æ˜¯ä»Šå¤©çš„è®¨è®ºç‚¹ï¼Œè¿™é‡Œå…ˆä¸åšè¯¦è¿°äº†ã€‚

#### å¹¿æ’­å˜é‡ä½œç”¨
å¹¿æ’­å˜é‡ç”¨æ¥é«˜æ•ˆåˆ†å‘è¾ƒå¤§çš„å¯¹è±¡ã€‚å‘æ‰€æœ‰å·¥ä½œèŠ‚ç‚¹å‘é€ä¸€ä¸ªè¾ƒå¤§çš„åªè¯»å€¼ï¼Œä»¥ä¾›ä¸€ä¸ªæˆ–å¤šä¸ªSparkæ“ä½œä½¿ç”¨ã€‚
æ¯”å¦‚ï¼Œå¦‚æœä½ çš„åº”ç”¨éœ€è¦å‘æ‰€æœ‰èŠ‚ç‚¹å‘é€ä¸€ä¸ªè¾ƒå¤§çš„åªè¯»æŸ¥è¯¢è¡¨ï¼Œå¹¿æ’­å˜é‡ç”¨èµ·æ¥éƒ½å¾ˆé¡ºæ‰‹ã€‚åœ¨å¤šä¸ªå¹¶è¡Œæ“ä½œä¸­ä½¿ç”¨åŒä¸€ä¸ªå˜é‡ï¼Œä½†æ˜¯ Sparkä¼šä¸ºæ¯ä¸ªä»»åŠ¡åˆ†åˆ«å‘é€

#### å¹¿æ’­å˜é‡ä½¿ç”¨æ­¥éª¤
1. è°ƒç”¨SparkContext.broadcastï¼ˆå¹¿æ’­å˜é‡ï¼‰åˆ›å»ºå‡ºä¸€ä¸ªå¹¿æ’­å¯¹è±¡ï¼Œä»»ä½•å¯åºåˆ—åŒ–çš„ç±»å‹éƒ½å¯ä»¥è¿™ä¹ˆå®ç°ã€‚  
2. é€šè¿‡å¹¿æ’­å˜é‡.valueï¼Œè®¿é—®è¯¥å¯¹è±¡çš„å€¼ã€‚  
3. å˜é‡åªä¼šè¢«å‘åˆ°å„ä¸ªèŠ‚ç‚¹ä¸€æ¬¡ï¼Œä½œä¸ºåªè¯»å€¼å¤„ç†ï¼ˆä¿®æ”¹è¿™ä¸ªå€¼ä¸ä¼šå½±å“åˆ°åˆ«çš„èŠ‚ç‚¹ï¼‰ã€‚  

#### å¹¿æ’­å˜é‡ä½¿ç”¨åœºæ™¯
joinä¼šå¯¼è‡´æ•°æ®é‡å‡ ä½•å¢é•¿ï¼Œå¹¶ä¸”ä¼šå½±å“shuffleçš„æ€§èƒ½ï¼Œä¸æ¨èä½¿ç”¨

#### ä¸ºä»€ä¹ˆä¼šå‡ºç°å¹¿æ’­å˜é‡
é—­åŒ…æ•°æ®ï¼Œæ˜¯ä»¥Taskä¸ºå•ä½å‘é€çš„ï¼Œæ¯ä¸ªä»»åŠ¡ä¸­åŒ…å«é—­åŒ…æ•°æ®è¿™æ ·å¯èƒ½ä¼šå¯¼è‡´ï¼Œä¸€ä¸ªExecutorä¸­åŒ…å«å¤§é‡é‡å¤æ•°æ®ï¼Œå¹¶ä¸”å ç”¨å¤§é‡å†…å­˜
Executorå…¶å®å°±æ˜¯ä¸€ä¸ªJVMï¼Œæ‰€ä»¥åœ¨å¯åŠ¨æ—¶ï¼Œä¼šè‡ªåŠ¨åˆ†é…å†…å­˜
å®Œå…¨å¯ä»¥å°†ä»»åŠ¡ä¸­çš„é—­åŒ…æ•°æ®æ”¾ç½®åœ¨Executorçš„å†…å­˜ä¸­ï¼Œè¾¾åˆ°å…±äº«çš„ç›®çš„  
Sparkä¸­çš„å¹¿æ’­å˜é‡å°±å¯ä»¥å°†é—­åŒ…çš„æ•°æ®ä¿å­˜åˆ°Executorçš„å†…å­˜ä¸­  
Sparkä¸­çš„å¹¿æ’­å˜é‡ä¸èƒ½æ›´æ”¹  


## Sparkä½œä¸šè¿è¡Œæµç¨‹
sparkåº”ç”¨ç¨‹åºä»¥è¿›ç¨‹é›†åˆä¸ºå•ä½åœ¨åˆ†å¸ƒå¼é›†ç¾¤ä¸Šè¿è¡Œï¼Œé€šè¿‡driverç¨‹åºçš„mainæ–¹æ³•åˆ›å»ºsparkContextçš„å¯¹è±¡ä¸é›†ç¾¤è¿›è¡Œäº¤äº’ã€‚å…·ä½“è¿è¡Œæµç¨‹å¦‚ä¸‹
1. sparkContextå‘cluster Managerç”³è¯·CPUï¼Œå†…å­˜ç­‰è®¡ç®—èµ„æºã€‚
2. cluster Manageråˆ†é…åº”ç”¨ç¨‹åºæ‰§è¡Œæ‰€éœ€è¦çš„èµ„æºï¼Œåœ¨workerèŠ‚ç‚¹åˆ›å»ºexecutorã€‚
3. sparkContextå°†ç¨‹åºä»£ç å’Œtaskä»»åŠ¡å‘é€åˆ°executorä¸Šè¿›è¡Œæ‰§è¡Œï¼Œä»£ç å¯ä»¥æ˜¯ç¼–è¯‘æˆçš„jaråŒ…æˆ–è€…pythonæ–‡ä»¶ç­‰ã€‚æ¥ç€sparkContextä¼šæ”¶é›†ç»“æœåˆ°Driverç«¯ã€‚


## Spark RDDè¿­ä»£è¿‡ç¨‹
1. sparkContextåˆ›å»ºRDDå¯¹è±¡ï¼Œè®¡ç®—RDDé—´çš„ä¾èµ–å…³ç³»ï¼Œå¹¶ç»„æˆä¸€ä¸ªDAGæœ‰å‘æ— ç¯å›¾ã€‚
2. DAGSchedulerå°†DAGåˆ’åˆ†ä¸ºå¤šä¸ªstageï¼Œå¹¶å°†stageå¯¹åº”çš„TaskSetæäº¤åˆ°é›†ç¾¤çš„ç®¡ç†ä¸­å¿ƒï¼Œstageçš„åˆ’åˆ†ä¾æ®æ˜¯RDDä¸­çš„å®½çª„ä¾èµ–ï¼Œ
	sparké‡è§å®½ä¾èµ–å°±ä¼šåˆ’åˆ†ä¸ºä¸€ä¸ªstageï¼Œæ¯ä¸ªstageä¸­åŒ…å«æ¥ä¸€ä¸ªæˆ–å¤šä¸ªtaskä»»åŠ¡ï¼Œé¿å…å¤šä¸ªstageä¹‹é—´æ¶ˆæ¯ä¼ é€’äº§ç”Ÿçš„ç³»ç»Ÿå¼€é”€ã€‚
3. taskScheduler é€šè¿‡é›†ç¾¤ç®¡ç†ä¸­å¿ƒä¸ºæ¯ä¸€ä¸ªtaskç”³è¯·èµ„æºå¹¶å°†taskæäº¤åˆ°workerçš„èŠ‚ç‚¹ä¸Šè¿›è¡Œæ‰§è¡Œã€‚
4. workerä¸Šçš„executoræ‰§è¡Œå…·ä½“çš„ä»»åŠ¡ã€‚


## Sparkæ ¸å¿ƒç¼–ç¨‹
RDDï¼šå¼¹æ€§åˆ†å¸ƒå¼æ•°æ®é›†
ç´¯åŠ å™¨ï¼šåˆ†å¸ƒå¼å…±äº«åªå†™å˜é‡
å¹¿æ’­å˜é‡ï¼šåˆ†å¸ƒå¼å…±äº«åªè¯»å˜é‡


## Spark RDD ç®—å­ä½¿ç”¨çš„åœºæ™¯
### ç»Ÿè®¡æ¯ä¸€ä¸ªçœä»½æ¯ä¸ªå¹¿å‘Šè¢«ç‚¹å‡»æ•°é‡æ’è¡Œçš„Top3  
[å‚è€ƒæ–‡æ¡£â€”â€”â€”â€”sparkç»Ÿè®¡æ¯ä¸ªçœä»½å¹¿å‘Šç‚¹å‡»é‡top3](https://www.cnblogs.com/dd110343/p/14319556.html)  
`æ—¶é—´æˆ³ï¼Œçœä»½ï¼ŒåŸå¸‚ï¼Œç”¨æˆ·ï¼Œå¹¿å‘Š`  ä¸­é—´å­—æ®µä½¿ç”¨é—´éš”çº¿åˆ†éš”ï¼Œç¤ºä¾‹`xxxxx - æ²³åŒ— - åŒ—äº¬ - å¼ ä¸‰ - A`
#### æ€è·¯
1. è·å–åŸå§‹æ•°æ®ï¼šæ—¶é—´æˆ³ã€çœä»½ã€åŸå¸‚ã€ç”¨æˆ·ã€å¹¿å‘Šç¼–å·  
2. å°†åŸå§‹æ•°æ®è½¬æ¢ä¸ºï¼ˆï¼ˆçœä»½ï¼Œå¹¿å‘Šï¼‰ï¼Œ1ï¼‰çš„å½¢å¼  
3. å°†è½¬æ¢åçš„æ•°æ®è¿›è¡Œèšåˆï¼ˆï¼ˆçœä»½ï¼Œå¹¿å‘Šï¼‰ï¼Œ1ï¼‰=>ï¼ˆï¼ˆçœä»½ï¼Œå¹¿å‘Šï¼‰ï¼Œsumï¼‰  
4. å°†èšåˆåçš„æ•°æ®è¿›è¡Œç»“æ„çš„è½¬æ¢ï¼šï¼ˆï¼ˆçœä»½ï¼Œå¹¿å‘Šï¼‰ï¼Œsumï¼‰=>ï¼ˆçœä»½ï¼Œï¼ˆå¹¿å‘Šï¼Œsumï¼‰ï¼‰  
5. å°†å¾—åˆ°çš„æ•°æ®æŒ‰ç…§çœä»½æ¥è¿›è¡Œåˆ†ç»„ï¼Œè½¬æ¢ä¸ºï¼ˆçœä»½ï¼Œï¼ˆå¹¿å‘Šï¼Œsumï¼‰ï¼Œï¼ˆå¹¿å‘Šï¼Œsumï¼‰ï¼Œï¼ˆå¹¿å‘Šï¼Œsumï¼‰... ï¼‰çš„æ ¼å¼  
6. å°†åˆ†ç»„åçš„æ•°æ®ç»„å†…æ’åºï¼ˆé™åºï¼‰ï¼Œå–å‰ä¸‰å  
#### ä»£ç 
```
package rdd.operator.transform

import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkConf, SparkContext}

object SparkTest {

  def main(args: Array[String]): Unit = {
    val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("Operator")
    val sc = new SparkContext(sparkConf)

    // 1.è·å–åŸå§‹æ•°æ®ï¼šæ—¶é—´æˆ³ã€çœä»½ã€åŸå¸‚ã€ç”¨æˆ·ã€å¹¿å‘Šç¼–å·
    val dataRDD: RDD[String] = sc.textFile("datas/agent.log")

    // 2.å°†åŸå§‹æ•°æ®è½¬æ¢ä¸ºï¼ˆï¼ˆçœä»½ï¼Œå¹¿å‘Šï¼‰ï¼Œ1ï¼‰çš„å½¢å¼
    val mapRDD: RDD[((String, String), Int)] = dataRDD.map(
      (line: String) => {
        val datas: Array[String] = line.split(" ")
        ((datas(1), datas(4)), 1)
      }
    )

    // 3.å°†è½¬æ¢åçš„æ•°æ®è¿›è¡Œèšåˆï¼ˆï¼ˆçœä»½ï¼Œå¹¿å‘Šï¼‰ï¼Œ1ï¼‰=>ï¼ˆï¼ˆçœä»½ï¼Œå¹¿å‘Šï¼‰ï¼Œsumï¼‰
    val reduceRDD: RDD[((String, String), Int)] = mapRDD.reduceByKey(_ + _)

    // 4.å°†èšåˆåçš„æ•°æ®è¿›è¡Œç»“æ„çš„è½¬æ¢ï¼šï¼ˆï¼ˆçœä»½ï¼Œå¹¿å‘Šï¼‰ï¼Œsumï¼‰=>ï¼ˆçœä»½ï¼Œï¼ˆå¹¿å‘Šï¼Œsumï¼‰ï¼‰
    val newmapRDD: RDD[(String, (String, Int))] = reduceRDD.map({
      case ((prv, ad), sum) => (prv, (ad, sum))
    })

    // 5.å°†å¾—åˆ°çš„æ•°æ®æŒ‰ç…§çœä»½æ¥è¿›è¡Œåˆ†ç»„
    val groupRDD: RDD[(String, Iterable[(String, Int)])] = newmapRDD.groupByKey()

    // 6.å°†åˆ†ç»„åçš„æ•°æ®ç»„å†…æ’åºï¼ˆé™åºï¼‰ï¼Œå–å‰ä¸‰å
    val resultRDD: RDD[(String, List[(String, Int)])] = groupRDD.map(
      str => {
        val list: List[(String, Int)] = str._2.toList.sortBy(_._2)(Ordering.Int.reverse).take(3)
        val sheng=str._1
        println("çœä»½ç¼–å·ï¼š"+sheng+"|å¹¿å‘Šç¼–å·ï¼š"+list.head._1+"  ç‚¹å‡»é‡ï¼š"+list.head._2+
          "|å¹¿å‘Šç¼–å·ï¼š"+list(1)._1+"  ç‚¹å‡»é‡ï¼š"+list(1)._2+
          "|å¹¿å‘Šç¼–å·ï¼š"+list(2)._1+"  ç‚¹å‡»é‡ï¼š"+list(2)._2)
        (str._1, list)
      }
    )

    /*val resultRDD: RDD[(String, List[(String, Int)])] = groupRDD.mapValues(
      (iter: Iterable[(String, Int)]) => {
        iter.toList.sortBy(_._2)(Ordering.Int.reverse).take(3)
      }
    )*/

    // 7.æ‰“å°ç»“æœ
    println("æ¯ä¸ªçœä»½å¯¹åº”å¹¿å‘Šç‚¹å‡»é‡å‰ä¸‰åï¼š")
    resultRDD.collect()

    sc.stop()
  }
}
```