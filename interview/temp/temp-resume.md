# 项目名称：SPR4生产数据分析系统
系统架构：Hadoop+Flume+Kafka+HBase+Hive+Sqoop+MySQL+Oozie+ZooKeeper+Tableau
开发工具：Centos7 + IDEA+ JDK1.8 + Maven
开发周期：12个月
项目描述：
在实际生产过程中，产品的报废率、机器设备的停机率、产品批次的差异型等因素已经对产品的合格率及产量产生巨大的影响；
公司为了提高产品质量，增加产品的合格率及对供应原料的调研，现在实际生产过程中增加了相关参数检测的传感设备，以实际生产过程为对象构建大数据平台，
通过建模等手段预测后续生产所需的准确参数。为了能够达到更好的数据分析效果，将以往的数据从追溯系统ACC中导出，通过分布式消息队列连接后端数据库进行离线分析。
底层使用多种存储结构，以满足不同的业务需求。整个系统通过ZooKeeper进行集群的管理和监控，保障集群的稳定运行。
该数据分析系统共有以下几个阶段：数据搜集、数据清洗、数据分析、数据建模和预测。通过近3年的工业现场的ACC追溯系统，
将以往生产数据导出，通过flume将这些数据拉取到Kafka的消息队列中。将Kafka消息队列中的数据导入到HBase中进行存储。
使用Hive创建数据仓库并进行数据清洗及特征集提取，将不同维度的数据存储到MySQL中。结合随机森林算法进行模型构建并对抽取的维度数据对模型进行训练，
将训练好的模型存储到HDFS中。将最近的生产生产数据作为预测数据，通过Spark Stream调用构建好的悬链模型进行数据预测。
#### 责任描述：
1. 参与项目的需求分析和项目构建；
2. 搭建项目的集群环境；
3. Flume整合Kafka，通过Flume集群将数据收集分发到Kafka；
4. 编写Java API，实现Hase消费Kafka中的数据；
5. 根据业务需求进行ETL数据特征集的抽取；
6. 问题处理以及Spark和hive相关优化

# 项目名称：昆钞回转大数据测试系统
系统架构：Flume+Hive+Hadoop+Zepplin+Sqoop
开发工具：Centos7 + IDEA+ JDK1.8
开发周期：6个月
项目描述：
本项目通过搭建Hadoop集群，将传统数据（Bom数据、PLM数据等）迁移至集群的平台上。通过Sqoop将原有的生产数据拉取出来，存储到HDFS中。
编写Sqoop Job定期进行增量导入，定期拉取变更MySQL中的数据进行存储。通过Flume监控指定本地Bom数据，导入值HDFS中。
通过使用Hive创建外表映射HDFS文件，通过Zepplin工具Hive表中数据进行查询、统计分析及数据展示等待。
#### 责任描述：
1. 参与Hadoop集群搭建与测试；
2. 参与项目的分析和优化；
3. 负责数据的采集、清洗和转换，整合海量数据处理流程；
4. 使用zepplin进行数据分析及表格呈现；

# 项目名称：SPR10生产数据分析系统
系统架构：Hadoop+Flume+Kafka+HBase+Hive+Sqoop+MySQL+Oozie+ZooKeeper+Tableau+Presto
开发工具：Centos7 + IDEA+ JDK1.8 + Maven
开发周期：8个月
项目描述：
本系统框架是基于SPR4数据分析系统的升级版本。在本系统中增加了Presto分布式SQL查询引擎，支持GB到PB字节的查询，提供快速查询。
通过构建大数据平、建模等手段预测后续生产所需的准确参数。为了能够达到更好的数据分析效果，将以往的数据从追溯系统ACC中导出，
通过分布式消息队列连接后端数据库进行离线分析。底层使用多种存储结构，以满足不同的业务需求。整个系统通过ZooKeeper进行集群的管理和监控，保障集群的稳定运行。
该数据分析系统共有以下几个阶段：数据搜集、数据清洗、数据分析、数据建模和预测。通过近3年的工业现场的ACC追溯系统，将以往生产数据导出，
通过flume将这些数据拉取到Kafka的消息队列中。将Kafka消息队列中的数据导入到HBase中进行存储。使用Hive创建数据仓库并进行数据清洗及特征集提取，
将不同维度的数据存储到MySQL中。结合随机森林算法进行模型构建并对抽取的维度数据对模型进行训练，将训练好的模型存储到HDFS中。
将最近的生产生产数据作为预测数据，通过Spark Stream调用构建好的悬链模型进行数据预测。
#### 责任描述：
1. 参与项目的需求分析和项目构建；
2. 搭建项目的集群环境；
3. Flume整合Kafka，通过Flume集群将日志收集分发到Kafka；
4. 编写Java API，实现Hase消费Kafka中的数据；
5. 根据业务需求进行ETL数据特征集的抽取；
6. 问题处理以及Spark和hive相关优化

# 项目名称：苏宁价格计算系统开发时间：2019.10-2021.11项目描述：苏宁价格系统是为了减轻业务端系统价格计算压力产生的大数据系统，随着易购平台各品类商品不断增加，促销活动不断增加，各种价格数据产生的量级业务系统已经无法承担价格计算的服务压力，价格计算系统是计算各种价格最终价格的任务执行系统，对底价，供价，促销价，到手价等进行系统计算，提供查询服务给各交易链路系统 #### 责任描述：1、参与需求分析、表结构设计的讨论；2、负责定价模块的编码与测试；3、负责编码与测试4、负责定价中心数据创建，下发，接口服务提供。#### 技术描述：1、每日通过苏宁大数据平台工具将业务系统数据迁移至hive 对原始数据进行清洗；2、通过spark rdd 和spark sql 对清洗后的数据进行计算处理，算得各渠道最终价格，并提供服务查询给下游调用链路；3、对现有任务进行版本迭代，并不断优化任务的执行4、通过flume对外围价格舆情信息日志进行采集至kafka 并对数据进行清洗分析，	提供查询给价格管理服务系统产生不同的价格定价规则模板。5、对价格端业务系统进行历史数据归档至hbase中，并为价格业务系统提供历史价格查询服务# 项目名称：苏宁中台自营价格家乐福数据迁移融合开发时间：2019.12-2021.11项目描述：该系统是苏宁价格中心为了对家乐福原始数据和苏宁原始数据进行融合应生的一个大数据平台系统，家乐福国内数据因为和苏宁是2套不同系统，数据内容与现有公司数据存有差异性，在公司收购了家乐福国内的项目后，需要对家乐福现有数据进行融合，该系统是对家乐福所有价格信息进行清洗，数据转换，逻辑计算得到与苏宁价格数据一致数据，并将最终事实数据通过kafka下发至价格业务端系统，由业务端对最终数据进行价格截断，完成数据迁移与融合#### 责任描述：1、负责项目前期的需求分析讨论；2、参与项目技术方案选型；3、Kafka下发接口设计与开发#### 技术描述：1、初始化全量数据阶段，通过spark获取hdfs中家乐福全量数据，负责集团限价和供价初始化任务，	通过spark rdd 对数据进行逐层清洗和数据转换，最终按照产品需求对转换后数据进行价格计算，	并将数据下发至kafka，业务端消费，并且进行业务端消费接收接口的开发处理2、配合其他中心，完成家乐福与苏宁数据的融合3、灰度阶段配合其他部门发布监控，并及时完成问题修复4、通过sparkstream 微批处理对家乐福每日增量数据进行 计算处理转换，并下发至业务系统