# 项目名称：SPR4生产数据分析系统
系统架构：Hadoop+Flume+Kafka+HBase+Hive+Sqoop+MySQL+Oozie+ZooKeeper+Tableau
开发工具：Centos7 + IDEA+ JDK1.8 + Maven
开发周期：12个月
项目描述：
在实际生产过程中，产品的报废率、机器设备的停机率、产品批次的差异型等因素已经对产品的合格率及产量产生巨大的影响；
公司为了提高产品质量，增加产品的合格率及对供应原料的调研，现在实际生产过程中增加了相关参数检测的传感设备，以实际生产过程为对象构建大数据平台，
通过建模等手段预测后续生产所需的准确参数。为了能够达到更好的数据分析效果，将以往的数据从追溯系统ACC中导出，通过分布式消息队列连接后端数据库进行离线分析。
底层使用多种存储结构，以满足不同的业务需求。整个系统通过ZooKeeper进行集群的管理和监控，保障集群的稳定运行。
该数据分析系统共有以下几个阶段：数据搜集、数据清洗、数据分析、数据建模和预测。通过近3年的工业现场的ACC追溯系统，
将以往生产数据导出，通过flume将这些数据拉取到Kafka的消息队列中。将Kafka消息队列中的数据导入到HBase中进行存储。
使用Hive创建数据仓库并进行数据清洗及特征集提取，将不同维度的数据存储到MySQL中。结合随机森林算法进行模型构建并对抽取的维度数据对模型进行训练，
将训练好的模型存储到HDFS中。将最近的生产生产数据作为预测数据，通过Spark Stream调用构建好的悬链模型进行数据预测。
#### 责任描述：
1. 参与项目的需求分析和项目构建；
2. 搭建项目的集群环境；
3. Flume整合Kafka，通过Flume集群将数据收集分发到Kafka；
4. 编写Java API，实现Hase消费Kafka中的数据；
5. 根据业务需求进行ETL数据特征集的抽取；
6. 问题处理以及Spark和hive相关优化

# 项目名称：昆钞回转大数据测试系统
系统架构：Flume+Hive+Hadoop+Zepplin+Sqoop
开发工具：Centos7 + IDEA+ JDK1.8
开发周期：6个月
项目描述：
本项目通过搭建Hadoop集群，将传统数据（Bom数据、PLM数据等）迁移至集群的平台上。通过Sqoop将原有的生产数据拉取出来，存储到HDFS中。
编写Sqoop Job定期进行增量导入，定期拉取变更MySQL中的数据进行存储。通过Flume监控指定本地Bom数据，导入值HDFS中。
通过使用Hive创建外表映射HDFS文件，通过Zepplin工具Hive表中数据进行查询、统计分析及数据展示等待。
#### 责任描述：
1. 参与Hadoop集群搭建与测试；
2. 参与项目的分析和优化；
3. 负责数据的采集、清洗和转换，整合海量数据处理流程；
4. 使用zepplin进行数据分析及表格呈现；

# 项目名称：SPR10生产数据分析系统
系统架构：Hadoop+Flume+Kafka+HBase+Hive+Sqoop+MySQL+Oozie+ZooKeeper+Tableau+Presto
开发工具：Centos7 + IDEA+ JDK1.8 + Maven
开发周期：8个月
项目描述：
本系统框架是基于SPR4数据分析系统的升级版本。在本系统中增加了Presto分布式SQL查询引擎，支持GB到PB字节的查询，提供快速查询。
通过构建大数据平、建模等手段预测后续生产所需的准确参数。为了能够达到更好的数据分析效果，将以往的数据从追溯系统ACC中导出，
通过分布式消息队列连接后端数据库进行离线分析。底层使用多种存储结构，以满足不同的业务需求。整个系统通过ZooKeeper进行集群的管理和监控，保障集群的稳定运行。
该数据分析系统共有以下几个阶段：数据搜集、数据清洗、数据分析、数据建模和预测。通过近3年的工业现场的ACC追溯系统，将以往生产数据导出，
通过flume将这些数据拉取到Kafka的消息队列中。将Kafka消息队列中的数据导入到HBase中进行存储。使用Hive创建数据仓库并进行数据清洗及特征集提取，
将不同维度的数据存储到MySQL中。结合随机森林算法进行模型构建并对抽取的维度数据对模型进行训练，将训练好的模型存储到HDFS中。
将最近的生产生产数据作为预测数据，通过Spark Stream调用构建好的悬链模型进行数据预测。
#### 责任描述：
1. 参与项目的需求分析和项目构建；
2. 搭建项目的集群环境；
3. Flume整合Kafka，通过Flume集群将日志收集分发到Kafka；
4. 编写Java API，实现Hase消费Kafka中的数据；
5. 根据业务需求进行ETL数据特征集的抽取；
6. 问题处理以及Spark和hive相关优化